{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-165de7c5d53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"diplom_test2.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1lCMapKzqQymHpHvYICnWnXUW3RzYBt06\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from matplotlib import pyplot as plt \n",
    "import math\n",
    "\n",
    "class deepRitz():\n",
    "\n",
    "    def __init__(self, spatial_range, num_hidden=20, batch_size=200, num_iters=1000, lr_rate=1e-3, output_path='deepRitz'):\n",
    "        self.spatial_range = spatial_range # for example, spatial range = [-1,1]\n",
    "        self.num_hidden = num_hidden\n",
    "        self.batch_size = batch_size #количество точек\n",
    "        self.num_iters = num_iters\n",
    "        self.lr_rate = lr_rate \n",
    "        self.output_path = output_path\n",
    "        self.loss_history = []\n",
    "        tf.reset_default_graph() \n",
    "        \n",
    "    def build_nn(self, input):\n",
    "        \"\"\"fcnet for all pde_type\"\"\"\n",
    "        num_hidden = self.num_hidden #количество нейронов внутри скрытых слоев (на одном слое)\n",
    "        with tf.variable_scope(\"build_nn\", reuse=tf.AUTO_REUSE): #создание переменных, повторное использование\n",
    "            x = tf.layers.Dense(num_hidden, activation=tf.nn.tanh)(input) #добавление слоя, гиперболический тангенс (-1,1)\n",
    "            x = tf.layers.Dense(num_hidden, activation=tf.nn.tanh)(x)\n",
    "            x = x + tf.layers.Dense(num_hidden, activation=tf.nn.tanh)(x) #метод, который позволяет лучше найти коэффициенты\n",
    "            x = tf.layers.Dense(num_hidden, activation=tf.nn.tanh)(x)\n",
    "            output = tf.layers.Dense(1)(x)\n",
    "        return output\n",
    "\n",
    "    def alfa(self,x):\n",
    "        a=self.spatial_range[0]\n",
    "        b=self.spatial_range[1]\n",
    "        return (x-a)*(b-x) #(x+1)(1-x)\n",
    "      \n",
    "    \n",
    "    def beta(self,x):\n",
    "        return 0\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"train step\"\"\"\n",
    "        \n",
    "        z = tf.placeholder(tf.float32, shape=[None,1]) #передаем n точек за раз [100,1]\n",
    "\n",
    "        u = self.alfa(z) * self.build_nn(z) + self.beta(z) #alpha(x)Net(x, theta) + beta(x)\n",
    "        \n",
    "        gradients = tf.gradients(u, z) #значение производной в соответствующей точке\n",
    "\n",
    "        slopes = tf.square(gradients) #возведение в квадрат\n",
    "            \n",
    "        def rightside(z): #f \n",
    "            return abs(z)/ z\n",
    "\n",
    "        loss = tf.reduce_mean(0.5*slopes-rightside(z)*u) #среднее значение (см пдф I(u))\n",
    "\n",
    "        \n",
    "        train_op = tf.train.AdamOptimizer(self.lr_rate).minimize(loss) #Параметры обучающей сети оптимизатора Адама\n",
    "        # train_op = tf.compat.v1.train.optimizers.SGD(self.lr_rate).minimize(loss)\n",
    "        \n",
    "        if os.path.exists(self.output_path):\n",
    "            shutil.rmtree(self.output_path)\n",
    "        os.mkdir(self.output_path)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess: #начав сессию вам необходимо ее завершить или использовать блок with… as, например вот так\n",
    "            sess.run(tf.global_variables_initializer()) #механизм для инициализации всех переменных за раз\n",
    "            for iter in range(self.num_iters):\n",
    "                #На каждом шаге “обучения” (оптимизации) эти точки выбираются заново (np.random.rand(self.batch_size, 1))\n",
    "                batch_data = self.spatial_range[0] + (self.spatial_range[1]-self.spatial_range[0])*np.random.rand(self.batch_size, 1)\n",
    "                loss_cur, _ = sess.run([loss, train_op], feed_dict={z: batch_data}) #feed_dict - передача данных\n",
    "                self.loss_history.append(loss_cur)\n",
    "                if (iter+1) % 100 == 0:\n",
    "                # if (iter+1) % 10 == 0:\n",
    "                    print('iteration: {}, loss: {:.4}'. format(iter+1, loss_cur))\n",
    "            saver.save(sess, os.path.join(self.output_path, \"model\"), global_step=iter)\n",
    "   \n",
    "    def test(self, x, branch):\n",
    "        checkpoint = tf.train.latest_checkpoint(self.output_path)\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            saver.restore(sess, checkpoint)\n",
    "            u = self.alfa(x) * self.build_nn(x) + self.beta(x) #приближенное решение\n",
    "            if branch==1:\n",
    "               u_test = sess.run(u)\n",
    "            else: #считаем вторую производную\n",
    "               gradients = tf.gradients(u, [x])[0] \n",
    "               ux=tf.reshape(gradients[:,0],[-1,1])\n",
    "               u_test = sess.run(ux)\n",
    "        return u_test #сеточная функция\n",
    "    \n",
    "    def numtest(self, num_points, branch=1):\n",
    "        xs = np.linspace(self.spatial_range[0],self.spatial_range[1], num_points, dtype=np.float32) #генерируем num_points точек в интервале \n",
    "        z=tf.reshape(xs,[-1,1])\n",
    "        return self.test(z, branch)\n",
    "\n",
    "spatial_range = [-1, 1]\n",
    "num_hidden = 40\n",
    "batch_size = 200 #количество точек\n",
    "num_iters = 100000\n",
    "lr_rate = 1e-3\n",
    "N = 41\n",
    "\n",
    "VarPDE = deepRitz(spatial_range, num_hidden, batch_size, num_iters, lr_rate)\n",
    "tic=time.time()\n",
    "VarPDE.train()\n",
    "toc=time.time()\n",
    "elapsed=toc-tic\n",
    "round(elapsed, 3)\n",
    "varloss_list = VarPDE.loss_history\n",
    "np.save('varloss_list.npy', varloss_list)\n",
    "\n",
    "varloss_list = VarPDE.loss_history\n",
    "np.save('varloss_list.npy', varloss_list)\n",
    "\n",
    "u=VarPDE.numtest(N)\n",
    "x = np.linspace(spatial_range[0], spatial_range[1], N, dtype=np.float32) #генерируем n точек в интервале [0,1]\n",
    "plt.plot(x,u)\n",
    "plt.title('Решение DNN')\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(spatial_range[0], spatial_range[1], N , dtype=np.float32) #генерируем n точек в интервале [0,1]\n",
    "plt.plot(x, u, \"r-\")\n",
    "x = np.linspace(spatial_range[0], 0, 41 , dtype=np.float32)\n",
    "plt.plot(x, 1/2*x*(x+1),\"g--\")\n",
    "x = np.linspace(0, spatial_range[1], 41 , dtype=np.float32) \n",
    "plt.plot(x, -1/2*x*(x-1),\"g--\")\n",
    "plt.legend(['NN', 'u'])\n",
    "# plt.title('lr_rate = 1e-2')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(varloss_list, 'b')\n",
    "plt.title('num_hidden = 40, batch_size = 10000')\n",
    "plt.show()\n",
    "\n",
    "ux=VarPDE.numtest(N, 2)\n",
    "x=np.linspace(spatial_range[0],spatial_range[1], N)\n",
    "plt.plot(x,ux,\"r-\")\n",
    "x = np.linspace(spatial_range[0], 0, 41 , dtype=np.float32)\n",
    "plt.plot(x, x+1/2,\"g--\")\n",
    "x = np.linspace(0, spatial_range[1], 41 , dtype=np.float32)\n",
    "plt.plot(x, -x+1/2,\"g--\")\n",
    "plt.legend(['NN', 'u\\''])\n",
    "plt.show()\n",
    "x=np.linspace(spatial_range[0],spatial_range[1], N)\n",
    "\n",
    "def exact2(x):\n",
    "  if x < spatial_range[0]:\n",
    "    return 1/2*x*(x+1)\n",
    "  elif spatial_range[0] <= x < 0:\n",
    "    return 1/2*x*(x+1)\n",
    "  elif 0 <= x <= spatial_range[1]:\n",
    "    return -1/2*x*(x-1)\n",
    "  elif x > spatial_range[1]:\n",
    "    return -1/2*x*(x-1)\n",
    "\n",
    "\n",
    "def exact_diff2(x):\n",
    "  if x < spatial_range[0]:\n",
    "    return x + 1/2\n",
    "  elif spatial_range[0] <= x < 0:\n",
    "    return x + 1/2\n",
    "  elif 0 <= x <= spatial_range[1]:\n",
    "    return -x + 1/2\n",
    "  elif x > spatial_range[1]:\n",
    "    return -x + 1/2\n",
    "\n",
    "def simpson(f, a, b):\n",
    "    val = (b - a) / 6 * (f(a) + 4 * f((a + b) / 2) + f(b))\n",
    "    return val\n",
    "\n",
    "def generalSimpson(f, a, b, eps):\n",
    "  M = 3\n",
    "  sum1 = simpson(f, a, b)\n",
    "  flag = True\n",
    "  while flag:\n",
    "    sum2 = 0\n",
    "    x = np.linspace(a, b, M)\n",
    "    for i in range(1, M):\n",
    "       sum2 += simpson(f, x[i-1], x[i])\n",
    "    flag = abs(sum1 - sum2) > 15*eps\n",
    "    sum1 = sum2\n",
    "    M = (M - 1) * 2 + 1\n",
    "  return sum2\n",
    "\n",
    "def count_integral_error(exact, exact_diff, a, step, approx):\n",
    "    diff = 0.0\n",
    "    ddiff = 0.0\n",
    "    v = a\n",
    "    for i in range(len(approx) - 1):\n",
    "        def _f(x):\n",
    "          val = exact(x) - (approx[i] * (v + step - x) + approx[i + 1] * (x - v)) / step\n",
    "          return val * val\n",
    "\n",
    "        def _f1(x):\n",
    "          val = exact_diff(x) - (approx[i + 1] - approx[i]) / step\n",
    "          return val * val\n",
    "\n",
    "        diff += generalSimpson(_f, v, v + step, 1e-8)\n",
    "        ddiff += generalSimpson(_f1, v, v + step, 1e-8)\n",
    "        v += step\n",
    "    return math.sqrt(diff), math.sqrt(ddiff)\n",
    "\n",
    "step = (spatial_range[1] - spatial_range[0]) / (N-1)\n",
    "e1, e2 = count_integral_error(exact2, exact_diff2, spatial_range[0], step, u)\n",
    "print(e1, e2)\n",
    "\n",
    "def count_norm(exact, exact_diff, a, b):\n",
    "  def _f(x):\n",
    "    val = exact(x)\n",
    "    return val * val\n",
    "\n",
    "  def _f1(x):\n",
    "    val = exact_diff(x)\n",
    "    return val * val\n",
    "\n",
    "  diff = generalSimpson(_f, a, b, 1e-8)\n",
    "  ddiff = generalSimpson(_f1, a, b, 1e-8)\n",
    "  return math.sqrt(diff), math.sqrt(ddiff)\n",
    "\n",
    "norm_u, norm_u_diff = count_norm(exact2, exact_diff2, spatial_range[0], spatial_range[1])\n",
    "print(norm_u, norm_u_diff)\n",
    "\n",
    "round(e1 / norm_u * 100, 2)\n",
    "\n",
    "round(e2 / norm_u_diff * 100, 2)\n",
    "\n",
    "y = [1000, 5000, 10000, 20000, 100000]\n",
    "# plt.plot(y, [11.02, 7.57, 8.72, 9.44])\n",
    "plt.plot(y, [11.29, 13.0, 5.98, 6.78, 4.08])\n",
    "# plt.plot(y, [21.08, 7.05, 6.79, 9.33])\n",
    "plt.legend(['Ошибка для производных при 10 нейронах', 'Ошибка для производных при 20 нейронах','Ошибка для производных при 40 нейронах'])\n",
    "plt.title('Зависимость ошибки от числа эпох, N=41')\n",
    "plt.show()\n",
    "\n",
    "y = [1000, 5000, 10000, 20000, 100000]\n",
    "# plt.plot(y, [11.33, 10.05, 6.2, 6.04])\n",
    "plt.plot(y, [11.42, 4.89, 9.08, 4.61, 5.68])\n",
    "# plt.plot(y, [12.53, 7.22, 5.86, 6.12])\n",
    "plt.legend(['Ошибка для производных при 10 нейронах', 'Ошибка для производных при 20 нейронах','Ошибка для производных при 40 нейронах'])\n",
    "plt.title('Зависимость ошибки от числа эпох, N=81')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
